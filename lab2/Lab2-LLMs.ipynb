{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f5d0b9d-7980-4d2c-8154-c07a5f8b5525",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this laboratory we will get our hands dirty working with Large Language Models (e.g. GPT and BERT) to do various useful things. I you haven't already, it is highly recommended to:\n",
    "\n",
    "+ Read the [Attention is All you Need](https://arxiv.org/abs/1706.03762) paper, which is the basis for all transformer-based LLMs.\n",
    "+ Watch (and potentially *code along*) with this [Andrej Karpathy video](https://www.youtube.com/watch?v=kCc8FmEb1nY) which shows you how to build an autoregressive GPT model from the ground up.\n",
    "\n",
    "# Exercise 1: Warming Up\n",
    "In this first exercise you will train a *small* autoregressive GPT model for character generation (the one used by Karpathy in his video) to generate text in the style of Dante Aligheri. Use [this file](https://archive.org/stream/ladivinacommedia00997gut/1ddcd09.txt), which contains the entire text of Dante's Inferno (**note**: you will have to delete some introductory text at the top of the file before training). Train the model for a few epochs, monitor the loss, and generate some text at the end of training. Qualitatively evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "from lab2.models.gpt import decode, estimate_loss, get_batch, GPTLanguageModel\n",
    "\n",
    "def make_paths_relative_to_root():\n",
    "    \"\"\"Always use the same, absolute (relative to root) paths\n",
    "    which makes moving the notebooks around easier.\n",
    "    \"\"\"\n",
    "    top_level = Path(__file__).parent\n",
    "    os.chdir(top_level)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "10.783546 M parameters\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "\n",
    "max_iters = 1500\n",
    "eval_interval = 100\n",
    "learning_rate = 3e-4\n",
    "\n",
    "eval_iters = 200 # how many iterations to use for evaluation\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 1.4181, val loss 1.5813\n",
      "step 100: train loss 1.3261, val loss 1.5519\n",
      "step 200: train loss 1.2284, val loss 1.5358\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 5\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28miter\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(max_iters):\n\u001B[0;32m      2\u001B[0m \n\u001B[0;32m      3\u001B[0m     \u001B[38;5;66;03m# every once in a while evaluate the loss on train and val sets\u001B[39;00m\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28miter\u001B[39m \u001B[38;5;241m%\u001B[39m eval_interval \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28miter\u001B[39m \u001B[38;5;241m==\u001B[39m max_iters \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m----> 5\u001B[0m         losses \u001B[38;5;241m=\u001B[39m \u001B[43mestimate_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstep \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28miter\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: train loss \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlosses[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, val loss \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlosses[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;66;03m# sample a batch of data\u001B[39;00m\n",
      "File \u001B[1;32m~\\mambaforge\\envs\\nlp\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "Cell \u001B[1;32mIn[12], line 64\u001B[0m, in \u001B[0;36mestimate_loss\u001B[1;34m()\u001B[0m\n\u001B[0;32m     62\u001B[0m         X, Y \u001B[38;5;241m=\u001B[39m get_batch(split)\n\u001B[0;32m     63\u001B[0m         logits, loss \u001B[38;5;241m=\u001B[39m model(X, Y)\n\u001B[1;32m---> 64\u001B[0m         losses[k] \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m     65\u001B[0m     out[split] \u001B[38;5;241m=\u001B[39m losses\u001B[38;5;241m.\u001B[39mmean()\n\u001B[0;32m     66\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "torch.save(model.state_dict(), 'gpt.pth' )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Udivi ai' dentro paro` che quei pion, speragia\n",
      "  vangiandom'io conorposto so buoro.\n",
      "\n",
      "Come farso lei gin.  Corto a costa partia:\n",
      "  Feggia` che ira dirro\n",
      "  - per la e scena Folgrimentici.\n",
      "\n",
      "<<O a milla diserrar ci tua calde>>.\n",
      "  dispirtoritto al suoi quel Sel eti cotiace!\n",
      "  zur dentro fingra infernon posa.\n",
      "\n",
      "<<I' volta che vol'omo sa sua quenta,\n",
      "  di Vuto di quando Sannomando` trai>>.\n",
      "\n",
      "Li` ne' si de' l folsen gralicevigio,\n",
      "  come i faron, avelto perro cossominci\n",
      "  e che io Alo mo,rte non largizz'amb\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "model.load_state_dict(torch.load('runs/nanogpt/gpt.pth'))\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
    "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-15T15:16:09.277610100Z",
     "start_time": "2023-06-15T15:15:45.485050800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "68441a09-dfaf-424a-b640-4fc8cea289b5",
   "metadata": {},
   "source": [
    "# Exercise 2: Working with Real LLMs\n",
    "\n",
    "Our toy GPT can only take us so far. In this exercise we will see how to use the [Hugging Face](https://huggingface.co/) model and dataset ecosystem to access a *huge* variety of pre-trained transformer models.\n",
    "\n",
    "## Exercise 2.1: Installation and text tokenization\n",
    "\n",
    "First things first, we need to install the [Hugging Face transformer library](https://huggingface.co/docs/transformers/index):\n",
    "\n",
    "    conda install -c huggingface -c conda-forge transformers\n",
    "    \n",
    "The key classes that you will work with are `GPT2Tokenizer` to encode text into sub-word tokens, and the `GPT2LMHeadModel`. **Note** the `LMHead` part of the class name -- this is the version of the GPT2 architecture that has the text prediction heads attached to the final hidden layer representations (i.e. what we need to **generate** text). \n",
    "\n",
    "Instantiate the `GPT2Tokenizer` and experiment with encoding text into integer tokens. Compare the length of input with the encoded sequence length.\n",
    "\n",
    "**Tip**: Pass the `return_tensors='pt'` argument to the togenizer to get Pytorch tensors as output (instead of lists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "af199a6d-1f3a-4b2c-a23f-d697b93c5adb",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-06-08T09:42:44.728249400Z",
     "start_time": "2023-06-08T09:42:42.073397500Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# import tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2',return_tensors='pt',padding_side='left')\n",
    "# define model backbone\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "GPT uses a BPE tokenizer, which is a subword-based tokenizer that allows to bring a good balance between character-level and word-level tokenizers, representing most common words as single tokens and rarer words as sequences of subword units.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21968, 15417, 32329, 481, 1716, 36834, 780]\n",
      "['Large', 'ĠLanguage', 'ĠModels', 'Ġwill', 'Ġbecome', 'Ġsentient', 'Ġbecause']\n"
     ]
    }
   ],
   "source": [
    "input = 'Large Language Models will become sentient because'\n",
    "\n",
    "input_ids = tokenizer.encode(input)\n",
    "print(input_ids)\n",
    "print(tokenizer.convert_ids_to_tokens(input_ids))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T09:42:44.758790900Z",
     "start_time": "2023-06-08T09:42:44.729244900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that some words are encoded with more than one token. All tokens containing the first word piece are preceded by the character Ġ"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "a458b725-63c1-49ae-8011-71a9196387b8",
   "metadata": {},
   "source": [
    "## Exercise 2.2: Generating Text\n",
    "\n",
    "There are a lot of ways we can, given a *prompt* in input, sample text from a GPT2 model. Instantiate a pre-trained `GPT2LMHeadModel` and use the [`generate()`](https://huggingface.co/docs/transformers/v4.27.2/en/main_classes/text_generation#transformers.GenerationMixin.generate) method to generate text from a prompt.\n",
    "\n",
    "**Note**: The default inference mode for GPT2 is *greedy* which might not results in satisfying generated text. Look at the `do_sample` and `temperature` parameters.\n",
    "\n",
    "### Greedy search\n",
    "In greedy search we always pick the token with the highest probability as the next token. This strategy can lead to repetitive and not very interesting text, as we can see in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bdad9208-cc9e-4750-baa5-f9367e71362a",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-06-08T09:42:48.505289100Z",
     "start_time": "2023-06-08T09:42:44.743253800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Large Language Models will become sentient because they will be able to communicate with each other.\n",
      "\n",
      "The AI will be able to communicate with the human race through the use of the language of the AI.\n",
      "\n",
      "The AI will be able to communicate with the human race through the use of the language of the AI. The AI will be able to communicate with the human race through the use of the language of the AI. The AI will be able to communicate with the human race through the use of the language of the AI. The AI will be able to communicate with the human race through the use of the language of the AI. The AI\n"
     ]
    }
   ],
   "source": [
    "# encode context the generation is conditioned on\n",
    "# don't pass anything\n",
    "input = tokenizer.bos_token\n",
    "# pass\n",
    "input = 'Large Language Models will become sentient because'\n",
    "\n",
    "input_ids = tokenizer.encode(input, return_tensors='pt')\n",
    "\n",
    "output = model.generate(input_ids, max_length=128)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multinomial sampling\n",
    "In multinomial sampling we randomly select the next token based on the probability distribution over the vocabulary. This allows to generate more diverse text, sampling less probable tokens. To perform multinomial sampling we set the `do_sample` parameter.\n",
    "\n",
    "We can control the diversity of the sampling by using the `temperature` parameter. A low temperature will result in a more conservative sampling, while a high temperature will result in a more diverse sampling."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Large Language Models will become sentient because of the fact that they are able to communicate with each other.\n",
      "\n",
      "The AI will be able to communicate with its own language, and will be able to communicate with other AI's language. The AI will be able to communicate with its own language, and will be able to communicate with other AI's language. The AI will be able to communicate with its own language, and will be able to communicate with other AI's language. The AI will be able to communicate with its own language, and will be able to communicate with other AI's language. The AI will be able to communicate with its own\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(input_ids,temperature=0.1,do_sample=True, max_length=128)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T09:42:52.120696700Z",
     "start_time": "2023-06-08T09:42:48.508280800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Large Language Models will become sentient because they have learned how to communicate with each others. This can become even more valuable in the future when these groups are both very interested in one another (and they often share many knowledge about the environment).\n",
      "\n",
      "Language models can also be used to show the difference between words in a world with different language systems – e.g. we can use the model to show all words are related to each other. Similarly, we can show which language is related to the other. Such language models can be used to show the differences between individuals to show that their language systems are related to them.\n",
      "\n",
      "These models can also be used to show how people think of the world infruit different from normal human speech. A good example of this can be seen with a simple language model that looks at a group of people who talk about the world in their normal everyday conversations. There are two types of this example – groups of people talking about the world in their normal everyday conversations and groups of people talking about the world in their normal everyday conversations. If you look at the picture, it starts to look like this:\n",
      "\n",
      "This is how the group of people talk about the world in their normal everyday conversations, even if these are not normal everyday conversations.\n",
      "\n",
      "A second kind of language model can be used for talking about specific concepts. For example, you can see how many concepts people have, for example \"I like you\" or \"I like your face\"\n",
      "\n",
      "Using language models to show the difference between ideas in this way also opens up the possibility to learn new ideas and to discover how to make new ideas. The new ideas can then be applied in the world of learning for a number of different reasons.\n",
      "\n",
      "Nowadays, the world has changed and is changing so rapidly. Many people have been taught how to think of this and it is very important to educate them, at least as a first step in learning new ideas. It is very important to encourage people to learn about this and other ideas as soon as possible.\n",
      "\n",
      "In addition, people are getting older. It happens that many people have to live in countries where they have to live longer periods of time, including before the 2000s. However, it does not seem very likely that they will be able to go back even the longest time.\n",
      "\n",
      "There are now more and more people trying to think of the world according to this concept. The number of people trying to get their ideas into the world can become quite large. However, it is possible\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(input_ids,temperature=0.9,do_sample=True,max_length=512)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T09:43:10.247161100Z",
     "start_time": "2023-06-08T09:42:52.123699400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Beam search\n",
    "In beam search we keep several sequence hypotheses at each time step, and eventually choose the one with the overall highest probability for the entire sequence. This has the advantage of identifying high-probability sequences that are not necessarily the most probable at each step. To perform beam search we set the `num_beams` parameter.\n",
    "\n",
    "We can also combine the beam search with multinomial sampling by setting the `do_sample` parameter."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Large Language Models will become sentient because of the way they interact with the world around them.\n",
      "\n",
      "In addition to the language models, there are many other models that can be used in order to interact with the world around them. These include:\n",
      "\n",
      "Human Language Models\n",
      "\n",
      "Human Language Models\n",
      "\n",
      "Human Language Models\n",
      "\n",
      "Human Language Models\n",
      "\n",
      "Human Language Models\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(input_ids, num_beams=5, temperature=0.9,do_sample=True, max_length=128)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T09:43:15.992448100Z",
     "start_time": "2023-06-08T09:43:10.251160400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "9b3d00e7-d4db-440f-8702-11118f07b0a4",
   "metadata": {},
   "source": [
    "# Exercise 3: Reusing Pre-trained LLMs (choose one)\n",
    "\n",
    "Choose **one** of the following exercises (well, *at least* one). In each of these you are asked to adapt a pre-trained LLM (`GPT2Model` or `DistillBERT` are two good choices) to a new Natural Language Understanding task. A few comments:\n",
    "\n",
    "+ Since GPT2 is a *autoregressive* model, there is no latent space aggregation at the last transformer layer (you get the same number of tokens out that you give in input). To use a pre-trained model for a classification or retrieval task, you should aggregate these tokens somehow (or opportunistically select *one* to use).\n",
    "\n",
    "+ BERT models (including DistillBERT) have a special [CLS] token prepended to each latent representation in output from a self-attention block. You can directly use this as a representation for classification (or retrieval).\n",
    "\n",
    "+ The first *two* exercises below can probably be done *without* any fine-tuning -- that is, just training a shallow MLP to classify or represent with the appropriate loss function.\n",
    "\n",
    "# Exercise 3.1: Training a Text Classifier (easy)\n",
    "\n",
    "Peruse the [text classification datasets on Hugging Face](https://huggingface.co/datasets?task_categories=task_categories:text-classification&sort=downloads). Choose a *moderately* sized dataset and use a LLM to train a classifier to solve the problem.\n",
    "\n",
    "**Note**: A good first baseline for this problem is certainly to use an LLM *exclusively* as a feature extractor and then train a shallow model.\n",
    "\n",
    "# Exercise 3.2: Training a Question Answering Model (harder)\n",
    "\n",
    "Peruse the [multiple choice question answering datasets on Hugging Face](https://huggingface.co/datasets?task_categories=task_categories:multiple-choice&sort=downloads). Chose a *moderately* sized one and train a model to answer contextualized multiple-choice questions. You *might* be able to avoid fine-tuning by training a simple model to *rank* the multiple choices (see margin ranking loss in Pytorch).\n",
    "\n",
    "# Exercise 3.3: Training a Retrieval Model (hardest)\n",
    "\n",
    "The Hugging Face dataset repository contains a large number of [\"text retrieval\" problems](https://huggingface.co/datasets?task_categories=task_categories:text-retrieval&p=1&sort=downloads). These tasks generally require that the model measure *similarity* between text in some metric space -- naively, just a cosine similarity between [CLS] tokens can get you pretty far. Find an interesting retrieval problem and train a model (starting from a pre-trained LLM of course) to solve it.\n",
    "\n",
    "**Tip**: Sometimes identifying the *retrieval* problems in these datasets can be half the challenge. [This dataset](https://huggingface.co/datasets/BeIR/scifact) might be a good starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Question Answering on Race Dataset\n",
    "Here I train a QA model on the [RACE](https://huggingface.co/datasets/race) dataset.\n",
    "The dataset is composed by 28K passages, and nearly 100K questions collected from English examinations in China, designed for middle and high school students."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-17T18:17:12.855626100Z",
     "start_time": "2023-06-17T18:17:10.692659700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset race (C:/Users/Dario/.cache/huggingface/datasets/race/middle/0.1.0/5839ff74a429622f5f20cca69c5fcf0e87ac6d5fd2777c42b948000684829f7b)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b3a7747ef414d7893d893dfeeca31cb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"race\",\"middle\")\n",
    "batch_size = 8"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-17T18:17:18.008083900Z",
     "start_time": "2023-06-17T18:17:12.861634600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 25421\n"
     ]
    }
   ],
   "source": [
    "train_data = dataset['train']\n",
    "val_data = dataset['validation']\n",
    "print(\"Train dataset length:\", len(train_data))\n",
    "dataiter = iter(val_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-17T18:17:28.075097300Z",
     "start_time": "2023-06-17T18:17:28.055103600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTEXT: APRIL 5 marks the 100thanniversary of the sinking of the Titanic. In 1997, the movieTitanicwas a huge hit all around the world. Now, the 3-D version of the movie will come to Chinese theaters on April 10.\n",
      "As anyone who has seen the movie knows, the Titanic struck an iceberg  and sank on its first _ in 1912, killing 1,517 people. A century after the accident, scientists have found something new to blame  for the sinking: the moon. Donald Olson, a Texas State University physicist, led a team of astronomers  to examine the moon's role, according to Reuters.\n",
      "Normally, icebergs stay in place and cannot move until they melt enough or a high tide  frees them. A single iceberg can become stuck many times on its journey southward. The process can take several years.\n",
      "According to Olson, a hundred years ago the moon made its closest approach to the Earth in more than 1,400 years. This caused the moon to have a much stronger pull on the Earth's oceans than usual, which created a super-high tide. T ...\n",
      "QUESTION: How many people lost their lives in this accident?\n",
      "OPTIONS: ['Over 2,000 people.', 'About 100 people.', 'More than 1,500 people.', \"We don't know.\"] - CORRECT: C\n"
     ]
    }
   ],
   "source": [
    "row = next(dataiter)\n",
    "# print(row)\n",
    "print(f\"CONTEXT: {row['article'][0:1000]} ...\")\n",
    "print(f\"QUESTION: {row['question']}\")\n",
    "print(f\"OPTIONS: {row['options']} - CORRECT: {row['answer']}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-17T18:17:29.178404400Z",
     "start_time": "2023-06-17T18:17:29.157400400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each row is composed by a **context**, a question and four possible options. Only one of them is the right one."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset race (C:/Users/Dario/.cache/huggingface/datasets/race/middle/0.1.0/5839ff74a429622f5f20cca69c5fcf0e87ac6d5fd2777c42b948000684829f7b)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2fed73e3e6e46bd8b6ee55bdabd9000"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset race (C:/Users/Dario/.cache/huggingface/datasets/race/middle/0.1.0/5839ff74a429622f5f20cca69c5fcf0e87ac6d5fd2777c42b948000684829f7b)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eb49b93a45b74aadaa7fc7e3a571715d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Dario\\.cache\\huggingface\\datasets\\race\\middle\\0.1.0\\5839ff74a429622f5f20cca69c5fcf0e87ac6d5fd2777c42b948000684829f7b\\cache-0034193d0877ac79.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Dario\\.cache\\huggingface\\datasets\\race\\middle\\0.1.0\\5839ff74a429622f5f20cca69c5fcf0e87ac6d5fd2777c42b948000684829f7b\\cache-8218c613ed21a888.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/1436 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8c83c66a57934f729cfb5e963e7f557f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lab2.data_loader.race_datamodule import RACEDataModule\n",
    "\n",
    "dm = RACEDataModule(\"distilbert-base-uncased\",\"middle\",512,batch_size,batch_size)\n",
    "\n",
    "dm.prepare_data()\n",
    "dm.setup(\"fit\")\n",
    "\n",
    "train_dl,valid_dl = dm.train_dataloader(), dm.val_dataloader()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-17T18:23:14.251660200Z",
     "start_time": "2023-06-17T18:23:02.088427100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([8, 4, 512])\n",
      "attention_mask torch.Size([8, 4, 512])\n",
      "label torch.Size([8])\n",
      "[CLS] april 5 marks the 100thanniversary of the sinking of the titanic. in 1997, the movietitanicwas a huge hit all around the world. now, the 3 - d version of the movie will come to chinese theaters on april 10. as anyone who has seen the movie knows, the titanic struck an iceberg and sank on its first _ in 1912, killing 1, 517 people. a century after the accident, scientists have found something new to blame for the sinking : the moon. donald olson, a texas state university physicist, led a team of astronomers to examine the moon's role, according to reuters. normally, icebergs stay in place and cannot move until they melt enough or a high tide frees them. a single iceberg can become stuck many times on its journey southward. the process can take several years. according to olson, a hundred years ago the moon made its closest approach to the earth in more than 1, 400 years. this caused the moon to have a much stronger pull on the earth's oceans than usual, which created a super - high tide. the tide pushed icebergs from shallow waters off the coasts of canada's provinces, newfoundland and labrador, into the titanic's way. \" of course, the final cause of the accident was that the ship struck an iceberg, \" olson told reuters. \" it went full speed into a region with icebergs, but thelunar connection may explain how an unusually large number of icebergs got into the path of the titanic. \" the research team will publish their research in the april issue of sky & telescope magazine.,,. [SEP] how many people lost their lives in this accident? over 2, 000 people. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(valid_dl)\n",
    "row = next(dataiter)\n",
    "\n",
    "for k,v in row.items():\n",
    "    print(k,v.shape)\n",
    "\n",
    "input_ids = row['input_ids']\n",
    "labels = row.pop(\"label\")\n",
    "\n",
    "print(dm.tokenizer.decode(input_ids[0,0,:]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-17T18:24:43.835786700Z",
     "start_time": "2023-06-17T18:24:43.772191800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from lab2.models.qa_lightning import QATransformer\n",
    "\n",
    "model = QATransformer(\"distilbert-base-uncased\",\n",
    "                      num_choices=4,\n",
    "                      criterion=\"CrossEntropyLoss\",\n",
    "                      task_name=\"middle\",\n",
    "                      learning_rate=1e-3\n",
    "                      )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-17T18:38:51.976913200Z",
     "start_time": "2023-06-17T18:38:50.732046600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature extraction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Margin ranking loss\n",
    "PyTorch's `MarginRankingLoss` is a loss function that aims to predict a relative distance between inputs. This task is called *Metric Learning*. It takes as input two inputs $x_1$ and $x_2$ and a label $y$ (containing 1 or -1).\n",
    "- If $y=1$ then $x_1$ should be ranked higher\n",
    "- Conversely, if $y=-1$ $x_2$ should have a higher value.\n",
    "\n",
    "The loss function is defined as follows:\n",
    "\n",
    "$$\\text{loss}(x_1, x_2, y) = \\max(0, -y * (x_1 - x_2) + \\text{margin})$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3739, grad_fn=<MultiMarginLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "logits = model(**row)\n",
    "criterion = F.multi_margin_loss\n",
    "\n",
    "loss = criterion(logits,labels,margin=0.5)\n",
    "\n",
    "print(loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-17T18:39:10.582239600Z",
     "start_time": "2023-06-17T18:38:55.296525400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Found cached dataset race (C:/Users/Dario/.cache/huggingface/datasets/race/middle/0.1.0/5839ff74a429622f5f20cca69c5fcf0e87ac6d5fd2777c42b948000684829f7b)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "826841c92e654f7b87a3236f4992bad1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset race (C:/Users/Dario/.cache/huggingface/datasets/race/middle/0.1.0/5839ff74a429622f5f20cca69c5fcf0e87ac6d5fd2777c42b948000684829f7b)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c908014fea854fcdaa5a2fd576c3786d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/1436 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e697afde3ac44af1a570710a90087fd1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/25421 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ea9f46fee1054c38b75153540a463fd9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/1436 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e1c1aea6467b4d36a1d7bb9e7e84af55"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type               | Params\n",
      "--------------------------------------------------\n",
      "0 | model      | DistilBertModel    | 66.4 M\n",
      "1 | classifier | Sequential         | 591 K \n",
      "2 | criterion  | CrossEntropyLoss   | 0     \n",
      "3 | train_acc  | MulticlassAccuracy | 0     \n",
      "4 | valid_acc  | MulticlassAccuracy | 0     \n",
      "--------------------------------------------------\n",
      "67.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "67.0 M    Total params\n",
      "267.817   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cefc47bbdd124aa49f8aaffcfcad9ae6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5680256d8bc746c983ad42f07e399cc8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import lightning\n",
    "import torch\n",
    "trainer = lightning.Trainer(\n",
    "    max_epochs=5,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1 if torch.cuda.is_available() else None,  # limiting got iPython runs\n",
    "    limit_train_batches=0.1,\n",
    "    limit_val_batches=0.1\n",
    ")\n",
    "trainer.fit(model, datamodule=dm)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-06-17T19:04:34.975268800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "april 5 marks the 100thanniversary of the sinking of the titanic. in 1997, the movietitanicwas a huge hit all around the world. now, the 3 - d version of the movie will come to chinese theaters on april 10. as anyone who has seen the movie knows, the titanic struck an iceberg and sank on its first _ in 1912, killing 1, 517 people. a century after the accident, scientists have found something new to blame for the sinking : the moon. donald olson, a texas state university physicist, led a team of astronomers to examine the moon's role, according to reuters. normally, icebergs stay in place and cannot move until they melt enough or a high tide frees them. a single iceberg can become stuck many times on its journey southward. the process can take several years. according to olson, a hundred years ago the moon made its closest approach to the earth in more than 1, 400 years. this caused the moon to have a much stronger pull on the earth's oceans than usual, which created a super - high tide. the tide pushed icebergs from shallow waters off the coasts of canada's provinces, newfoundland and labrador, into the titanic's way. \" of course, the final cause of the accident was that the ship struck an iceberg, \" olson told reuters. \" it went full speed into a region with icebergs, but thelunar connection may explain how an unusually large number of icebergs got into the path of the titanic. \" the research team will publish their research in the april issue of sky & telescope magazine.,,.. what is the article mainly about? more than 1, 500 people.\n",
      "tensor([2, 1, 3, 1, 2, 0, 1, 2]) tensor([1, 0, 1, 3, 0, 3, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(race_datamodule.val_dataloader()))\n",
    "ys = batch.pop(\"label\")\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "labels = torch.index_select(batch['input_ids'], 1, ys)[:,0,:]\n",
    "\n",
    "print(race_datamodule.tokenizer.batch_decode(labels,skip_special_tokens=True)[0])\n",
    "\n",
    "batch = {k:v.to(device)for k,v in batch.items()}\n",
    "\n",
    "preds = torch.argmax(model(**batch) , dim=1)\n",
    "\n",
    "print(ys,preds)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-17T12:26:44.395398900Z",
     "start_time": "2023-06-17T12:26:25.754203900Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
