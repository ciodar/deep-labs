{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f5d0b9d-7980-4d2c-8154-c07a5f8b5525",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this laboratory we will get our hands dirty working with Large Language Models (e.g. GPT and BERT) to do various useful things. I you haven't already, it is highly recommended to:\n",
    "\n",
    "+ Read the [Attention is All you Need](https://arxiv.org/abs/1706.03762) paper, which is the basis for all transformer-based LLMs.\n",
    "+ Watch (and potentially *code along*) with this [Andrej Karpathy video](https://www.youtube.com/watch?v=kCc8FmEb1nY) which shows you how to build an autoregressive GPT model from the ground up.\n",
    "\n",
    "# Exercise 1: Warming Up\n",
    "In this first exercise you will train a *small* autoregressive GPT model for character generation (the one used by Karpathy in his video) to generate text in the style of Dante Aligheri. Use [this file](https://archive.org/stream/ladivinacommedia00997gut/1ddcd09.txt), which contains the entire text of Dante's Inferno (**note**: you will have to delete some introductory text at the top of the file before training). Train the model for a few epochs, monitor the loss, and generate some text at the end of training. Qualitatively evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T20:03:05.436805400Z",
     "start_time": "2023-06-18T20:02:57.189783700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pathlib as pl\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from data_loader.swag_datamodule import SWAGDataModule\n",
    "from data_loader.feature_datamodule import FeatureDataModule\n",
    "from feature_extraction import extract_features\n",
    "\n",
    "from models import QAMLP\n",
    "from models.gpt import decode, estimate_loss, get_batch, GPTLanguageModel\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "10.783546 M parameters\n"
     ]
    }
   ],
   "source": [
    "max_iters = 500\n",
    "eval_interval = 100\n",
    "learning_rate = 3e-4\n",
    "\n",
    "checkpoint_path = 'runs/nanogpt/gpt.pth'\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = GPTLanguageModel()\n",
    "model = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from runs/nanogpt/gpt.pth\n",
      "step 0: train loss 1.7579, val loss 1.8074\n",
      "step 100: train loss 1.6759, val loss 1.7364\n",
      "step 200: train loss 1.5785, val loss 1.6650\n",
      "step 300: train loss 1.4732, val loss 1.6043\n",
      "step 400: train loss 1.3904, val loss 1.5724\n",
      "step 499: train loss 1.2913, val loss 1.5445\n"
     ]
    }
   ],
   "source": [
    "if pl.Path(checkpoint_path).exists():\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# overwrite the checkpoint\n",
    "torch.save(model.state_dict(), checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T15:16:09.277610100Z",
     "start_time": "2023-06-15T15:15:45.485050800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "E che 'l botta` d'io mai poco altedesmo;\n",
      "  paro` che' idia ma l'ospeta e 'l diviola.\n",
      "\n",
      "Quel l'e` quasi unl'alla sempo sco,\n",
      "  tecesser nia la bolle di ben Stese,\n",
      "  fiati suoleta de lor mano i calde.\n",
      "\n",
      "Noi gido` che fate d'io le seguante,\n",
      "  con la` d'occh'l penti foco scia ' lieto;\n",
      "  e ch'ancosi` per stiglier de l'ittrossia.\n",
      "\n",
      "I' me, ch'a piu` toffestiro dirsi,\n",
      "  e desssu prianzi fierati il liro,\n",
      "  tanno poi che nosti sanza fonti:\n",
      "\n",
      "ch'ebber di reperosso e iolti arebbia\n",
      "  vannor misini ornivi unos in \n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "model.load_state_dict(torch.load('runs/nanogpt/gpt.pth'))\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n",
    "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79499e05",
   "metadata": {},
   "source": [
    "We can see that after 1000 steps text stats to resemble the style of Divina Commedia, even though the words used aren't real. Some relationships, like angular brackets for citations and the title of the \"Canto\" are also learned."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68441a09-dfaf-424a-b640-4fc8cea289b5",
   "metadata": {},
   "source": [
    "# Exercise 2: Working with Real LLMs\n",
    "\n",
    "Our toy GPT can only take us so far. In this exercise we will see how to use the [Hugging Face](https://huggingface.co/) model and dataset ecosystem to access a *huge* variety of pre-trained transformer models.\n",
    "\n",
    "## Exercise 2.1: Installation and text tokenization\n",
    "\n",
    "First things first, we need to install the [Hugging Face transformer library](https://huggingface.co/docs/transformers/index):\n",
    "\n",
    "    conda install -c huggingface -c conda-forge transformers\n",
    "    \n",
    "The key classes that you will work with are `GPT2Tokenizer` to encode text into sub-word tokens, and the `GPT2LMHeadModel`. **Note** the `LMHead` part of the class name -- this is the version of the GPT2 architecture that has the text prediction heads attached to the final hidden layer representations (i.e. what we need to **generate** text). \n",
    "\n",
    "Instantiate the `GPT2Tokenizer` and experiment with encoding text into integer tokens. Compare the length of input with the encoded sequence length.\n",
    "\n",
    "**Tip**: Pass the `return_tensors='pt'` argument to the togenizer to get Pytorch tensors as output (instead of lists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af199a6d-1f3a-4b2c-a23f-d697b93c5adb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T09:42:44.728249400Z",
     "start_time": "2023-06-08T09:42:42.073397500Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e4f4d7af99a48c79ff6356b7d6c7161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dario\\mambaforge\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Dario\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e56dfc7e884ba8b28c10a0157befa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a04936ac0849a282112167f76ed6e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22093ecf4294e85b50e2b16916b34cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a678e13990f94b298ada5f90e7cdd568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# import tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2',return_tensors='pt',padding_side='left')\n",
    "# define model backbone\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "GPT uses a **BPE tokenizer**, which is a subword-based tokenizer that allows to bring a good balance between character-level and word-level tokenizers, representing most common words as single tokens and rarer words as sequences of subword units.\n",
    "\n",
    "To see this in action, let's try to encode the following sentence:\n",
    "\n",
    "    \"The quick brown fox jumps over the lazy dog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T09:42:44.758790900Z",
     "start_time": "2023-06-08T09:42:44.729244900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[464, 2068, 7586, 21831, 18045, 625, 262, 16931, 3290]\n",
      "['The', 'Ä quick', 'Ä brown', 'Ä fox', 'Ä jumps', 'Ä over', 'Ä the', 'Ä lazy', 'Ä dog']\n"
     ]
    }
   ],
   "source": [
    "input = 'The quick brown fox jumps over the lazy dog'\n",
    "\n",
    "input_ids = tokenizer.encode(input)\n",
    "print(input_ids)\n",
    "print(tokenizer.convert_ids_to_tokens(input_ids))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can see that some words are encoded with more than one token. All tokens containing the first word piece are preceded by the character Ä "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a458b725-63c1-49ae-8011-71a9196387b8",
   "metadata": {},
   "source": [
    "## Exercise 2.2: Generating Text\n",
    "\n",
    "There are a lot of ways we can, given a *prompt* in input, sample text from a GPT2 model. Instantiate a pre-trained `GPT2LMHeadModel` and use the [`generate()`](https://huggingface.co/docs/transformers/v4.27.2/en/main_classes/text_generation#transformers.GenerationMixin.generate) method to generate text from a prompt.\n",
    "\n",
    "**Note**: The default inference mode for GPT2 is *greedy* which might not results in satisfying generated text. Look at the `do_sample` and `temperature` parameters.\n",
    "\n",
    "### Greedy search\n",
    "In greedy search we always pick the token with the highest probability as the next token. This strategy can lead to repetitive and not very interesting text, as we can see in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdad9208-cc9e-4750-baa5-f9367e71362a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T09:42:48.505289100Z",
     "start_time": "2023-06-08T09:42:44.743253800Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The quick brown fox jumps over the lazy dog and runs off.\n",
      "\n",
      "\"I'm sorry, I'm sorry, I'm sorry,\" the fox says.\n",
      "\n",
      "\"I'm sorry, I'm sorry, I'm sorry,\" the fox says.\n",
      "\n",
      "\"I'm sorry, I'm sorry, I'm sorry,\" the fox says.\n",
      "\n",
      "\"I'm sorry, I'm sorry, I'm sorry,\" the fox says.\n",
      "\n",
      "\"I'm sorry, I'm sorry, I'm sorry,\" the fox says.\n",
      "\n",
      "\"I'm sorry, I'm sorry, I'm sorry,\" the fox says.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# encode context the generation is conditioned on\n",
    "# don't pass anything\n",
    "#input = tokenizer.bos_token\n",
    "\n",
    "input_ids = tokenizer.encode(input, return_tensors='pt')\n",
    "\n",
    "output = model.generate(input_ids, max_length=128)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Multinomial sampling\n",
    "In multinomial sampling we randomly select the next token based on the probability distribution over the vocabulary. This allows to generate more diverse text, sampling less probable tokens. To perform multinomial sampling we set the `do_sample` parameter.\n",
    "\n",
    "We can control the diversity of the sampling by using the `temperature` parameter. A low temperature will result in a more conservative sampling, while a high temperature will result in a more diverse sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78e27f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The quick brown fox jumps over the lazy dog and runs his paws along his body. The lazy dog keeps his head down and he waits for the fox to run his paws at him. I don't want to be done with the dog. I wish he wouldn't run around on his little paws. The fox comes to rest between the lazy dog and my little fox and he looks at the other dog and says, \"I'm a puppy\". The lazy dog starts to run away from me and I take my leave and go to sleep somewhere. I want to go home, but I don't have to sleep. This is the truth of life or at least, the truth of my life.\n",
      "\n",
      "Sometimes times I really want to sleep, because I'm so sad. My mother took me to a very long time after school and told me to wait for a while. I thought it would take all of my willpower to wait for a little bit more. This seems the answer to all my worries. I always say to myself, \"Maybe I'll come back a little later then\". I say, \"No\".\n",
      "\n",
      "You want to stay up late every night feeling sad after a long day of watching TV and reading books. A good day is the best time to sit at a desk or watch TV. The TV is so good because everyone is using it. As I sat in my sleep I could not get out the door. I thought, \"Maybe I'll just sit here thinking in my head that I can get out of bed and I'll find a seat and sit in it\". As I found the seat I felt like I was in the center of a tornado and only now the whole world saw me. I felt like it would never end! I was completely lost in the world. I knew I would never find another way to find peace and love. It is very clear that I could never find love, ever!\n",
      "\n",
      "One way to make myself feel happy while watching TV is to put myself under the supervision of people who will watch. This is not too hard, but as always I will stop right here and go back to my life. A good person will help you find your path of love. If you really want to find your life you must be in a relationship as the man or woman who is right for you, that person must be your new master.\n",
      "\n",
      "If you are watching TV and you are not sure you want to go, try this simple method. If yourpointers will be low it may be time to\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(input_ids,temperature=0.9,do_sample=True,max_length=512)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e0e4804",
   "metadata": {},
   "source": [
    "By lowering the temperature, we can see that the text becomes more repetitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T09:42:52.120696700Z",
     "start_time": "2023-06-08T09:42:48.508280800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The quick brown fox jumps over the lazy dog and runs off.\n",
      "\n",
      "The lazy dog runs off.\n",
      "\n",
      "The lazy dog runs off.\n",
      "\n",
      "The lazy dog runs off.\n",
      "\n",
      "The lazy dog runs off.\n",
      "\n",
      "The lazy dog runs off.\n",
      "\n",
      "The lazy dog runs off.\n",
      "\n",
      "The lazy dog runs off.\n",
      " seafood\n",
      "\n",
      "The lazy dog runs off.\n",
      "\n",
      "The lazy dog runs off.\n",
      "\n",
      "The lazy dog runs offtch\n",
      "\n",
      "The lazy dog runs off.\n",
      "\n",
      "The lazy dog runs off.\n",
      "\n",
      "The lazy dog runs off.\n",
      "\n",
      "The lazy dog runs off.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(input_ids,temperature=0.1,do_sample=True, max_length=128)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Beam search\n",
    "In beam search we keep several sequence hypotheses at each time step, and eventually choose the one with the overall highest probability for the entire sequence. This has the advantage of identifying high-probability sequences that are not necessarily the most probable at each step. To perform beam search we set the `num_beams` parameter.\n",
    "\n",
    "We can also combine the beam search with multinomial sampling by setting the `do_sample` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b22ea542",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T09:43:15.992448100Z",
     "start_time": "2023-06-08T09:43:10.251160400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The quick brown fox jumps over the lazy dog and starts to run.\n",
      "\n",
      "\"Hey, look at that!\"\n",
      "\n",
      "The lazy dog starts running.\n",
      "\n",
      "\"Hey, look at that!\"\n",
      "\n",
      "The lazy dog starts running.\n",
      "\n",
      "\"Hey, look at that!\"\n",
      "\n",
      "The lazy dog starts running.\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(input_ids, num_beams=5, temperature=0.9,do_sample=True, max_length=128)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88f1b069",
   "metadata": {},
   "source": [
    "Another way to avoid repetitions is to use the `no_repeat_ngram_size` parameter, which prevents the model from generating the same n-gram twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92ece69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The quick brown fox jumps over the lazy dog and runs off.\n",
      "\n",
      "\"I'm not going to let you go,\" she says. \"I'm going to tell you to get out of here.\"\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(input_ids, num_beams=5, temperature=0.9,do_sample=True, no_repeat_ngram_size=3, max_length=128)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b3d00e7-d4db-440f-8702-11118f07b0a4",
   "metadata": {},
   "source": [
    "# Exercise 3: Reusing Pre-trained LLMs (choose one)\n",
    "\n",
    "Choose **one** of the following exercises (well, *at least* one). In each of these you are asked to adapt a pre-trained LLM (`GPT2Model` or `DistillBERT` are two good choices) to a new Natural Language Understanding task. A few comments:\n",
    "\n",
    "+ Since GPT2 is a *autoregressive* model, there is no latent space aggregation at the last transformer layer (you get the same number of tokens out that you give in input). To use a pre-trained model for a classification or retrieval task, you should aggregate these tokens somehow (or opportunistically select *one* to use).\n",
    "\n",
    "+ BERT models (including DistillBERT) have a special [CLS] token prepended to each latent representation in output from a self-attention block. You can directly use this as a representation for classification (or retrieval).\n",
    "\n",
    "+ The first *two* exercises below can probably be done *without* any fine-tuning -- that is, just training a shallow MLP to classify or represent with the appropriate loss function.\n",
    "\n",
    "## Exercise 3.1: Training a Text Classifier (easy)\n",
    "\n",
    "Peruse the [text classification datasets on Hugging Face](https://huggingface.co/datasets?task_categories=task_categories:text-classification&sort=downloads). Choose a *moderately* sized dataset and use a LLM to train a classifier to solve the problem.\n",
    "\n",
    "**Note**: A good first baseline for this problem is certainly to use an LLM *exclusively* as a feature extractor and then train a shallow model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d66740a8",
   "metadata": {},
   "source": [
    "### Irony detection on tweet-eval\n",
    "\n",
    "[![WandB report](https://img.shields.io/badge/Weights_&_Biases-FFCC33?style=plastic&logo=WeightsAndBiases&logoColor=black)]()\n",
    "\n",
    "Humor is a difficult concept. GPT-4 can [explain jokes](./images/gpt4-jokes.png), even if it takes all the fun out of them in the process.\n",
    "\n",
    "Let's see if we can train a simple model to detect ironic tweets. We will use the [tweet-eval irony susbset](https://huggingface.co/datasets/tweet_eval/viewer/irony/train?p=0) from ðŸ¤— Datasets.\n",
    "\n",
    "This dataset is already split into train, validation and test sets, with 2.86k training samples, 784 validation samples and 955 test samples. Each sample is a tweet with a label indicating whether the tweet is ironic or not.\n",
    "\n",
    "The dataset is already preprocessed, stripping out mentions and URLs. Emojis are kept as tokens.\n",
    "\n",
    "We will use the `distilbert-base-uncased` model, which is a smaller version of the BERT model. We will use the ðŸ¤— Transformers library to load the model and the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "181cf7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset tweet_eval (C:/Users/Dario/.cache/huggingface/datasets/tweet_eval/irony/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff691353d42a4ed4a820feac900b4aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'seeing ppl walking w/ crutches makes me really excited for the next 3 weeks of my life', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('tweet_eval', 'irony')\n",
    "\n",
    "train_data = dataset['train']\n",
    "val_data = dataset['validation']\n",
    "test_data = dataset['test']\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print(train_data[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30746e44",
   "metadata": {},
   "source": [
    "It is useful to know some rough statistics about the length of each sentence in the dataset, in order to choose the maximum length of the input sequence for the tokenizer, and evaluate if we need to truncate some sentences.\n",
    "In this case, the maximum length is 177 words, but the mean is 14, and by setting the maximum length to 30 we can cover 95% of the sentences without truncation, while lowering a bit the memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "895bba62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length: 177\n",
      "Min length: 1\n",
      "Mean length: 13.732884155618343\n",
      "Standard deviation: 6.379859818821617\n"
     ]
    }
   ],
   "source": [
    "all_lengths = []\n",
    "\n",
    "for split in [train_data, val_data, test_data]:\n",
    "    for item in split:\n",
    "        all_lengths.append(len(item['text'].split()))\n",
    "\n",
    "print(f\"Max length: {max(all_lengths)}\")\n",
    "print(f\"Min length: {min(all_lengths)}\")\n",
    "print(f\"Mean length: {sum(all_lengths) / len(all_lengths)}\")\n",
    "print(f\"Standard deviation: {np.std(all_lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2299b732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset tweet_eval (C:/Users/Dario/.cache/huggingface/datasets/tweet_eval/irony/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec97ac21f434e5b9d83bf3e9943a093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset tweet_eval (C:/Users/Dario/.cache/huggingface/datasets/tweet_eval/irony/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df20b971e0ce42c3b6dd9f1aff19f6e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Dario\\.cache\\huggingface\\datasets\\tweet_eval\\irony\\1.1.0\\12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343\\cache-27298fa162cbec90.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Dario\\.cache\\huggingface\\datasets\\tweet_eval\\irony\\1.1.0\\12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343\\cache-cba6f32e375d5951.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e08d4357eb485581788ad3aa9ce712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/955 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dario\\mambaforge\\envs\\nlp\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from data_loader import TweetEvalDataModule\n",
    "\n",
    "te_dm = TweetEvalDataModule(model_name_or_path=\"distilbert-base-uncased\", task_name=\"irony\",max_seq_length=30, train_batch_size=BATCH_SIZE, eval_batch_size=BATCH_SIZE)\n",
    "\n",
    "te_dm.prepare_data()\n",
    "te_dm.setup(\"fit\")\n",
    "\n",
    "te_train_dl,te_val_dl = te_dm.train_dataloader(),te_dm.val_dataloader()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08e2a7d2",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "We employed a pre-trained [DistilBERT](https://arxiv.org/pdf/1910.01108.pdf) model from HuggingFace's Transformers library.\n",
    "DistilBERT is a distilled version of BERT, with 40% less parameters than BERT-base, and runs 60% faster while preserving over 95% of BERT's performances, mneasuered on GLUE language understanding benchmark. \n",
    "We will use this model as a feature extractor, and we will train a shallow MLP to perform a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceab1f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a86603ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': torch.Size([32, 30]), 'attention_mask': torch.Size([32, 30]), 'label': torch.Size([32])}\n"
     ]
    }
   ],
   "source": [
    "for batch in te_train_dl:\n",
    "    batch = {k:v.squeeze() for k,v in batch.items()}\n",
    "    print({k:v.shape for k,v in batch.items()})\n",
    "    label = batch.pop(\"label\")\n",
    "\n",
    "    out = model(**batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a642126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = FeatureDataModule('./data','tweet_eval',BATCH_SIZE,num_workers=6,pin_memory=True)\n",
    "\n",
    "dm.prepare_data()\n",
    "dm.setup(\"fit\")\n",
    "\n",
    "train_dl,valid_dl = dm.train_dataloader(), dm.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e764ed01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type           | Params\n",
      "----------------------------------------------\n",
      "0 | classifier | Sequential     | 197 K \n",
      "1 | criterion  | BCELoss        | 0     \n",
      "2 | train_acc  | BinaryAccuracy | 0     \n",
      "3 | valid_acc  | BinaryAccuracy | 0     \n",
      "----------------------------------------------\n",
      "197 K     Trainable params\n",
      "0         Non-trainable params\n",
      "197 K     Total params\n",
      "0.788     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ff14f315f44ceb8366cc4876a22252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe58020ff584f7891febf72192df1ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e44c456a6643a181b9b2c6b333a4e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68200fb523a14e72b90540f5dd284cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17fdcb0f63b94b0a854763103fff50aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085480c36b804df48b607cfbef8dfb38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf9cdaaf2ad4300ba643bcf26aa3415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5668c809fbd740ea98a1285fe3f85c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cae662787d943f799232e376b4b0501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8aa08f301744b8799f3f69b12cb7808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd830ee857294eeb8580ac5b57cd4323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92245b0109d54091bbbaacb0e4c91252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20628e8d48eb484f877abf6bff1fd260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfafb292ef8b45468ace49864677e54f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5ffeff5d3e947f780a42891b6bbc66d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638a47d09e3142cc87c2a4871e166864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b5e14608c2d43438af31f44f235e7b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e467ec8b5b426fb92e4a04cf48d216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28d131adbec143e1aed8b00f603effbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b173f040595941e09bea542c346668d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e023b312975b4e6aafc8aa7dea50e8fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92b1a22fa53c4d5e83ab08e5a8719747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    }
   ],
   "source": [
    "from models import MLP\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "CRITERION = \"BCELoss\"\n",
    "EPOCHS = 20\n",
    "\n",
    "model = MLP(num_classes=1,input_size=768,hidden_size=256,criterion=CRITERION,learning_rate=LEARNING_RATE)\n",
    "\n",
    "from lightning import Trainer\n",
    "trainer = Trainer(max_epochs=EPOCHS, devices=1, logger=False,enable_checkpointing=False)\n",
    "trainer.fit(model, train_dl, valid_dl)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efedf1a2",
   "metadata": {},
   "source": [
    "## Exercise 3.2: Training a Question Answering Model (harder)\n",
    "\n",
    "Peruse the [multiple choice question answering datasets on Hugging Face](https://huggingface.co/datasets?task_categories=task_categories:multiple-choice&sort=downloads). Chose a *moderately* sized one and train a model to answer contextualized multiple-choice questions. You *might* be able to avoid fine-tuning by training a simple model to *rank* the multiple choices (see margin ranking loss in Pytorch).\n",
    "\n",
    "### SWAG Dataset\n",
    "\n",
    "[![WandB report](https://img.shields.io/badge/Weights_&_Biases-FFCC33?style=plastic&logo=WeightsAndBiases&logoColor=black)](https://api.wandb.ai/links/dla-darcio/25op1oeh)\n",
    "\n",
    "We chose the [SWAG](https://huggingface.co/datasets/swag) dataset for this task.\n",
    "\n",
    "The dataset is composed by 113k passages(73k training, 20k validation, 20k test),with four answer choices about what might happen next in the scene. The correct answer is the (real) video caption for the next event in the video; the three incorrect answers are adversarially generated and human verified, so as to fool machines but not humans. SWAG aims to be a benchmark for evaluating grounded commonsense NLI and for learning representations. \n",
    "\n",
    "<!-- Another dataset I explored is [RACE](https://huggingface.co/datasets/race) dataset. -->\n",
    "<!-- The dataset is composed by 28K passages, and nearly 100K questions collected from English examinations in China, designed for middle and high school students. -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc0a832b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T20:03:10.496800500Z",
     "start_time": "2023-06-18T20:03:05.426801200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset swag (C:/Users/Dario/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb1a65c6a87451881b818020dc03992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"swag\",\"regular\")\n",
    "\n",
    "# hyperparameters\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b15a5f41",
   "metadata": {},
   "source": [
    "\n",
    "Each sample is composed by a complete sentence, the start of the next sentence and four possible continuations. Only one of them is the right one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e670de79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 73546\n"
     ]
    }
   ],
   "source": [
    "train_data = dataset['train']\n",
    "val_data = dataset['validation']\n",
    "print(\"Train dataset length:\", len(train_data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cee5f0c9",
   "metadata": {},
   "source": [
    "Let's print an example from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29444101",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T18:17:28.075097300Z",
     "start_time": "2023-06-17T18:17:28.055103600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTEXT: Students lower their eyes nervously. She\n",
      "ENDING: 0: pats her shoulder, then saunters toward someone.\n",
      "ENDING: 1: turns with two students.\n",
      "ENDING: 2: walks slowly towards someone.\n",
      "ENDING: 3: wheels around as her dog thunders out.\n",
      "CORRECT: 2\n"
     ]
    }
   ],
   "source": [
    "endings = ['ending0', 'ending1', 'ending2', 'ending3']\n",
    "row = val_data[0]\n",
    "# print(row)\n",
    "print(f\"CONTEXT: {row['startphrase']}\")\n",
    "for i, c in enumerate(endings):\n",
    "    print(f\"ENDING: {i}: {row[c]}\")\n",
    "print(f\"CORRECT: {row['label']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "620c148e",
   "metadata": {},
   "source": [
    "As done in the previous exercise, let's see some statistics about the length of the sentences in the dataset. The maximum length is 101 words, but the mean is 14, and by setting again the maximum length to 30 we can cover 95% of the sentences without truncation, while lowering a bit the memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a5a7052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of startphrase: 14.06\n",
      "Standard deviation of startphrase: 6.74\n",
      "Max length of startphrase: 101\n",
      "Min length of startphrase: 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "lengths = list(map(lambda x: len(x.split()),train_data['startphrase']))\n",
    "print(f\"Average length of startphrase: {np.mean(lengths):.2f}\")\n",
    "print(f\"Standard deviation of startphrase: {np.std(lengths):.2f}\")\n",
    "print(\"Max length of startphrase:\", np.max(lengths))\n",
    "print(\"Min length of startphrase:\", np.min(lengths))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(lengths, bins=20)\n",
    "plt.xlabel(\"Number of words\")\n",
    "plt.ylabel(\"Number of examples\")\n",
    "plt.title(\"Distribution of example lengths in the SWAG dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62fdb012",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "To feed these samples to a model, we need to find an efficient encoding of the sentences. \n",
    "Our preprocessing pipeline will be composed by two steps:\n",
    "\n",
    "1. Sentence construction: we will build sentence pairs formed by `sent1` and the concatenation of `sent2` and each of the four possible continuations. This means that we will obtain four difference sentence pairs for each sample.\n",
    "\n",
    "2. Tokenization: we will encode each sentence pair using the tokenizer provided by the model. Between the two sentences we will add a special token `[SEP]`, which is used by the model to distinguish between the two sentences. We will also have a special token `[CLS]` at the beginning of the first sentence, which contains the representation of the entire sentence pair. To build batches, we will pad each sample to the maximum length of the batch, adding the special token `[PAD]`.\n",
    "\n",
    "### âš¡Lightning DataModules \n",
    "\n",
    "To keep code tidy and improve experiment repeatability, I usedâš¡Lightning's `LightningDataModule` to define the dataset, perform  preprocessing and the dataloaders. \n",
    "The `prepare_data` method is used to download the dataset and the Tokenizer, while the `setup` method is used to split the dataset into train, validation and test sets and perform preprocessing, tokenization and encoding.\n",
    "\n",
    "The processed dataset is composed by the following fields:\n",
    "- `input_ids`: tokenized and encoded text (context + question + options)\n",
    "- `attention_mask`: attention mask to avoid performing attention on padding tokens\n",
    "- `label`: index of the correct answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f829898e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-27T21:33:10.210195900Z",
     "start_time": "2023-06-27T21:33:09.809196500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset swag (C:/Users/Dario/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95cb514d71294a7abfbe78f28a812a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset swag (C:/Users/Dario/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a26b97713f64593a4cf91fe46f3ab33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee38a0689cd74691bcf7262877998bfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/73546 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dario\\mambaforge\\envs\\nlp\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37964931af604286b80bdfedf5125fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20006 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64923b44475646ba8c27c43f1ea708b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20005 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "swag_dm = SWAGDataModule(\"distilbert-base-uncased\",\"regular\",30,BATCH_SIZE,BATCH_SIZE)\n",
    "swag_dm.prepare_data()\n",
    "swag_dm.setup(\"fit\")\n",
    "\n",
    "swag_train_dl,swag_valid_dl = swag_dm.train_dataloader(), swag_dm.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c44f8219",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-27T21:33:10.207199400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([32, 4, 30])\n",
      "attention_mask torch.Size([32, 4, 30])\n",
      "label torch.Size([32])\n",
      "[CLS] a man is in a bike shop and stand next to a bike while talking. [SEP] the man begins working out on his bike. [SEP] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "for row in swag_valid_dl:\n",
    "    for k,v in row.items():\n",
    "        print(k,v.shape)\n",
    "\n",
    "    input_ids = row['input_ids']\n",
    "    attention_mask = row['attention_mask']\n",
    "    labels = row.pop(\"label\")\n",
    "\n",
    "    print(swag_dm.tokenizer.decode(input_ids[4,3,:]))\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07550eb0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Model\n",
    "\n",
    "As in previous exercise, we will use the DistilBERT model. One possible issue is that DistilBERT was not trained on the Next Sentence Prediction task during the distillation process, but it was shown in [RoBERTa](https://arxiv.org/pdf/1907.11692.pdf) paper that this task may not be necessary for Natural Language Inference tasks.\n",
    "\n",
    "The MLP used for the downstream task is composed by two linear layers with ReLU activation and dropout. Since DistilBERT's hidden size is 768,  The first layer will have 768 input units and $d$ output units, while the second layer has $d$ input units and 1 output unit, with $d$ being a hyperparameter.\n",
    "We chose $d=256$ in our experiments.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6adf4dff",
   "metadata": {},
   "source": [
    "### Feature extraction\n",
    "\n",
    "The output of the model is the computed hidden state, a 768-dimensional vector for each token in the input sequence. We will use the pooled representation, available in the initial `[CLS]` token as a representation of the entire input sequence. \n",
    "The pooled representation is computed by averaging the hidden states of the last four layers of the model by default.\n",
    "\n",
    "We will pre-compute the hidden state for each sample in the dataset, and store in a file. This will speed up the training process, since we will not need to compute the hidden state for each sample at each epoch.\n",
    "The pre-computed hidden states (1GB) and are available at this [link](https://drive.google.com/drive/folders/1vjHMx2viF16B2NrA7WKOPV8GbQb9s-M8?usp=sharing) \n",
    "\n",
    "You can re-compute the hidden states by running the following command:\n",
    "\n",
    "```bash\n",
    "python extract_features.py --model_name_or_path distilbert-base-uncased --max_seq_length 32 --batch_size <YOUR_BATCH_SIZE(default=32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1f5d32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = FeatureDataModule('./data','swag',BATCH_SIZE,num_workers=6,pin_memory=True)\n",
    "\n",
    "dm.prepare_data()\n",
    "dm.setup(\"fit\")\n",
    "\n",
    "train_dl,valid_dl = dm.train_dataloader(), dm.val_dataloader()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd814449",
   "metadata": {},
   "source": [
    "#### Classification setting \n",
    "\n",
    "The classifier is used to predict the probability of each answer choice to be the correct one. That mean that the model will evaluate the \"correctness\" of each value indipendently, without comparing the alternatives. \n",
    "The model is then trained with the cross entropy loss. I believe that this setting is simple, but it is harder to learn and to get a good performance.\n",
    "\n",
    "Surprisingly, this is the approach also used in the HuggingFace implementation `DistilBertForMultipleChoice`. If you're curious, you can find the code [here](https://github.com/huggingface/transformers/blob/04f46a22d8b5f38c8369b3d995df3102b8d56dcd/src/transformers/models/distilbert/modeling_distilbert.py#L1033). This approach may scale badly to a higher number of alternatives, since the model will need to compute the probability of each alternative independently."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69a2a340",
   "metadata": {},
   "source": [
    "I define a custom dataset FeatureDataset to load the pre-computed hidden states and the labels. I suspect that the Dataloader approach is equivalent to split the tensor into batches and cycle through them, but I'll use the DataLoader approach."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "513609fc",
   "metadata": {},
   "source": [
    "An interesting alternative for classification could be to give to the MLP an input composed by the concatenation of the pooled representations of each alternative."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b909965e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Margin ranking loss\n",
    "PyTorch's `MarginRankingLoss` is a loss function that aims to predict a relative distance between inputs. This task is called *Metric Learning*. It takes as input two inputs $x_1$ and $x_2$ and a label $y$ (containing 1 or -1).\n",
    "- If $y=1$ then $x_1$ should be ranked higher\n",
    "- Conversely, if $y=-1$ $x_2$ should have a higher value.\n",
    "\n",
    "The loss function is defined as follows:\n",
    "\n",
    "$$\\text{loss}(x_1, x_2, y) = \\max(0, -y * (x_1 - x_2) + \\text{margin})$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff74b901",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Training\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f24319ff",
   "metadata": {},
   "source": [
    "Training is performed by running the following script:\n",
    "\n",
    "```bash\n",
    "python main.py fit -c configs/YOUR_CONFIG_FILE.yaml\n",
    "```\n",
    "\n",
    "A list of configuration file is available in [configs](./configs/) folder.\n",
    "Alternatively, here's a simplified training snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae8dbc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type               | Params\n",
      "--------------------------------------------------\n",
      "0 | classifier | Sequential         | 197 K \n",
      "1 | criterion  | CrossEntropyLoss   | 0     \n",
      "2 | train_acc  | MulticlassAccuracy | 0     \n",
      "3 | valid_acc  | MulticlassAccuracy | 0     \n",
      "--------------------------------------------------\n",
      "197 K     Trainable params\n",
      "0         Non-trainable params\n",
      "197 K     Total params\n",
      "0.788     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a471f4e641840c8b2a02d751f1431e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e783850c584eb19902a9ae4a2a7833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99fb1fce4b324843b4cb18c1363b2f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df06de2516f4442a99ea918c3449aae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d80616ff5754be8a898eca8d1abf86d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2190b78470d4ee28dc28cf439f7dd42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af9d557f72a482ea3ed94ed4fb5ccf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a5d9735fd34327991897fb0271ff75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c1ad80f364e4d42a09e5d61ebf5f65c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1fef3faf5bf4e1b9bbb8c0c398eb618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9166ae3de89940249ea6b8c1a4081bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf7d0dcf4d241df93e481a53d695d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d313763a724b8b81af9b949d7c38cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3bdf96d6fd540c2913a364ad170a4b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163d214907c84a9fb0312e3272ac2f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "347c2cf7f75b43fc80ace6434de17744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b654c0363548466c87393da07519511f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03a7ccb28a54674a9e176a51b8463d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8516490c3e1342f4a8271ad250b6ad3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85b3ba0d253441587aca93107c356ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc8edb2cd484b628496d52447f32a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d14dbbd50449868d9912db8f8aa268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "CRITERION = \"CrossEntropyLoss\"\n",
    "NUM_CHOICES = 4\n",
    "EPOCHS = 20\n",
    "\n",
    "model = QAMLP(num_choices=NUM_CHOICES,input_size=768,hidden_size=256,criterion=CRITERION,learning_rate=LEARNING_RATE)\n",
    "\n",
    "from lightning import Trainer\n",
    "trainer = Trainer(max_epochs=EPOCHS, devices=1, logger=False,enable_checkpointing=False)\n",
    "trainer.fit(model, train_dl, valid_dl)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0bfa74b5",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "A detailed report is available on [WandB](https://wandb.ai//dla-darcio/lab-2-question-answering/reports/Multiple-Choice-Question-Answering--Vmlldzo0NzY0MTM3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "119ff6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "students lower their eyes nervously. she pats her shoulder, then saunters toward someone.\n",
      "students lower their eyes nervously. she turns with two students.\n",
      "students lower their eyes nervously. she walks slowly towards someone.\n",
      "students lower their eyes nervously. she wheels around as her dog thunders out.\n",
      "CORRECT: 2\n",
      "Features shape: torch.Size([32, 4, 768])\n",
      "PREDICTED: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bert = AutoModel.from_pretrained(\"distilbert-base-uncased\").to(device)\n",
    "bert.eval()\n",
    "\n",
    "for batch in swag_valid_dl:\n",
    "\n",
    "    decoded_text = swag_dm.tokenizer.batch_decode(batch['input_ids'][0],skip_special_tokens=True)\n",
    "\n",
    "    for i in range(4):\n",
    "        print(decoded_text[i])\n",
    "    print(f\"CORRECT: {batch['label'][0]}\")\n",
    "\n",
    "    features, labels = extract_features(bert,device=device,batch=batch)\n",
    "\n",
    "    print(f\"Features shape: {features.shape}\")\n",
    "\n",
    "    logits = model(features)\n",
    "\n",
    "    probs = nn.functional.softmax(logits, dim=1)\n",
    "\n",
    "    print(f\"PREDICTED: {torch.argmax(logits[0,:])}\")\n",
    "\n",
    "    plt.bar(np.arange(4), probs[0,:].detach().cpu().numpy())\n",
    "    plt.xlabel(\"Choice\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.title(\"Predicted probabilities\")\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7721c869",
   "metadata": {},
   "source": [
    "## Exercise 3.3: Training a Retrieval Model (hardest)\n",
    "\n",
    "The Hugging Face dataset repository contains a large number of [\"text retrieval\" problems](https://huggingface.co/datasets?task_categories=task_categories:text-retrieval&p=1&sort=downloads). These tasks generally require that the model measure *similarity* between text in some metric space -- naively, just a cosine similarity between [CLS] tokens can get you pretty far. Find an interesting retrieval problem and train a model (starting from a pre-trained LLM of course) to solve it.\n",
    "\n",
    "**Tip**: Sometimes identifying the *retrieval* problems in these datasets can be half the challenge. [This dataset](https://huggingface.co/datasets/BeIR/scifact) might be a good starting point.\n",
    "\n",
    "## References\n",
    "\n",
    "- [HuggingFace Transformers](https://huggingface.co/transformers/)\n",
    "- [HuggingFace Datasets](https://huggingface.co/datasets)\n",
    "- [PyTorch Lightning](https://www.pytorchlightning.ai/)\n",
    "- [PyTorch Lightning Trainer](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html)\n",
    "- [PyTorch Lightning LightningDataModule](https://pytorch-lightning.readthedocs.io/en/latest/extensions/datamodules.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
